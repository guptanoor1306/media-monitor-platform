#!/usr/bin/env python3
"""
Fix SSL issues and populate with working RSS feeds
"""

import asyncio
import aiohttp
import feedparser
import sys
import os
import ssl
from datetime import datetime, timezone, timedelta
from urllib.parse import urljoin
import re

# Add the parent directory to the path so we can import our modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.database import SessionLocal, init_db
from src.models import Source, Content, Summary

# Working RSS feeds that don't have SSL issues
WORKING_FEEDS = {
    'blog': {
        # HTTP feeds (no SSL issues)
        'Hacker News': 'http://feeds.feedburner.com/ycombinator',
        'Ars Technica': 'http://feeds.arstechnica.com/arstechnica/index',
        'TechCrunch': 'http://feeds.feedburner.com/TechCrunch/',
        'The Verge': 'http://www.theverge.com/rss/index.xml',
        'Wired': 'http://feeds.wired.com/wired/index',
        'VentureBeat': 'http://feeds.venturebeat.com/VentureBeat',
        'Mashable': 'http://feeds.mashable.com/Mashable',
        'TechnoStory': 'http://www.technostellar.com/feed/',
        'Engadget': 'http://www.engadget.com/rss.xml',
        
        # Business feeds
        'Harvard Business Review': 'http://feeds.hbr.org/harvardbusiness',
        'Fast Company': 'http://feeds.fastcompany.com/fastcompany',
        'Entrepreneur': 'http://feeds.feedburner.com/entrepreneur/latest',
        'Inc.com': 'http://www.inc.com/rss',
        
        # Creator/Media feeds  
        'Medium Technology': 'http://medium.com/feed/topic/technology',
        'Medium Startup': 'http://medium.com/feed/topic/startup',
        'Medium Business': 'http://medium.com/feed/topic/business',
        'Reddit Entrepreneur': 'http://www.reddit.com/r/entrepreneur/.rss',
        'Reddit Startup': 'http://www.reddit.com/r/startups/.rss',
        'Reddit Business': 'http://www.reddit.com/r/business/.rss',
    },
    
    'podcast': {
        # Podcast feeds via HTTP
        'NPR How I Built This': 'http://www.npr.org/rss/podcast.php?id=510313',
        'Tim Ferriss Show': 'http://feeds.feedburner.com/thetimferrissshow',  
        'Entrepreneur on Fire': 'http://feeds.feedburner.com/eofire',
        'Smart Passive Income': 'http://feeds.feedburner.com/smartpassiveincome',
        'Masters of Scale': 'http://feeds.megaphone.fm/mastersofscale',
        'The GaryVee Audio Experience': 'http://feeds.feedburner.com/garyvaynerchuk',
    },
    
    'news': {
        # News feeds via HTTP
        'Reuters': 'http://feeds.reuters.com/reuters/businessNews',
        'BBC Business': 'http://feeds.bbci.co.uk/news/business/rss.xml',
        'CNN Business': 'http://rss.cnn.com/rss/money_latest.rss',
        'MarketWatch': 'http://feeds.marketwatch.com/marketwatch/bulletins/',
        'Yahoo Finance': 'http://finance.yahoo.com/rss/headline',
    },
    
    'newsletter': {
        # Newsletter-style feeds
        'Morning Brew': 'http://morningbrew.com/rss',
        'The Hustle': 'http://feeds.feedburner.com/thehustle',
        'CB Insights': 'http://feeds.feedburner.com/cbinsights',
    }
}

async def fetch_rss_content_with_ssl_fix(session, name, url, limit_days=365):
    """Fetch RSS content with SSL verification disabled"""
    try:
        print(f"ğŸ“¡ Fetching: {name}")
        
        # Try with SSL verification disabled
        connector = aiohttp.TCPConnector(ssl=False)
        async with aiohttp.ClientSession(connector=connector) as ssl_session:
            async with ssl_session.get(url, timeout=30) as response:
                if response.status != 200:
                    print(f"âŒ HTTP {response.status} for {name}")
                    return []
                
                content = await response.text()
        
        # Parse RSS feed
        feed = feedparser.parse(content)
        
        if not hasattr(feed, 'entries') or not feed.entries:
            print(f"âš ï¸  No entries found in {name}")
            return []
        
        # Filter for recent content (last year)
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=limit_days)
        recent_entries = []
        
        for entry in feed.entries:
            # Parse entry date
            entry_date = None
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                try:
                    entry_date = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                except:
                    pass
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                try:
                    entry_date = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
                except:
                    pass
            
            # Only include recent entries or entries without dates
            if entry_date and entry_date >= cutoff_date:
                recent_entries.append(entry)
            elif not entry_date:  # Include entries without dates
                recent_entries.append(entry)
        
        print(f"âœ… {name}: {len(recent_entries)} recent items from {len(feed.entries)} total")
        return recent_entries
        
    except Exception as e:
        print(f"âŒ Error fetching {name}: {str(e)}")
        return []

async def create_working_sources():
    """Create working RSS sources in database"""
    db = SessionLocal()
    
    try:
        print("ğŸ§¹ Clearing existing sources...")
        # Delete summaries first (foreign key constraint)
        db.query(Summary).delete()
        # Delete content (foreign key constraint)  
        db.query(Content).delete()
        # Delete sources
        db.query(Source).delete()
        db.commit()
        print("âœ… Cleared all existing sources and content")
        
        print("\nğŸ“‚ Adding working RSS sources...")
        total_added = 0
        
        for source_type, sources in WORKING_FEEDS.items():
            print(f"\nğŸ“ Adding {source_type.upper()} sources:")
            
            for name, url in sources.items():
                source = Source(
                    name=name,
                    url=url,
                    source_type=source_type,
                    description=f"Working RSS feed for {name}",
                    is_active=True,
                    update_interval=3600  # 1 hour
                )
                db.add(source)
                print(f"  âœ… {name}")
                total_added += 1
        
        db.commit()
        print(f"\nğŸ‰ Added {total_added} working sources!")
        return total_added
        
    except Exception as e:
        print(f"âŒ Error creating sources: {e}")
        db.rollback()
        return 0
    finally:
        db.close()

async def scrape_all_working_content():
    """Scrape content from all working sources"""
    db = SessionLocal()
    
    try:
        print("\nğŸ“° Scraping content from working sources...")
        
        # Get all sources
        sources = db.query(Source).all()
        
        total_content = 0
        
        for source in sources:
            print(f"\nğŸ”„ Processing: {source.name}")
            
            try:
                # Create session with SSL disabled
                connector = aiohttp.TCPConnector(ssl=False)
                async with aiohttp.ClientSession(connector=connector) as session:
                    # Fetch RSS entries
                    entries = await fetch_rss_content_with_ssl_fix(session, source.name, source.url)
                
                if not entries:
                    continue
                
                # Convert entries to Content objects
                added_count = 0
                for entry in entries:
                    try:
                        # Extract content details
                        title = getattr(entry, 'title', 'No Title')
                        link = getattr(entry, 'link', '')
                        description = getattr(entry, 'summary', getattr(entry, 'description', ''))
                        
                        # Clean up HTML tags from description
                        if description:
                            import re
                            description = re.sub(r'<[^>]+>', '', description)
                            description = description.strip()
                        
                        # Parse published date
                        published_at = None
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            try:
                                published_at = datetime(*entry.published_parsed[:6])
                            except:
                                published_at = datetime.now()
                        elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                            try:
                                published_at = datetime(*entry.updated_parsed[:6])
                            except:
                                published_at = datetime.now()
                        else:
                            published_at = datetime.now()
                        
                        # Extract author
                        author = getattr(entry, 'author', None)
                        
                        # Generate tags based on content
                        tags = []
                        content_text = (title + " " + description).lower()
                        
                        # Add intelligent tags
                        if any(word in content_text for word in ['startup', 'funding', 'venture', 'vc', 'investment']):
                            tags.append('startup_funding')
                        if any(word in content_text for word in ['creator', 'influencer', 'youtube', 'tiktok', 'instagram']):
                            tags.append('creator_economy')
                        if any(word in content_text for word in ['ai', 'artificial intelligence', 'machine learning', 'deep learning']):
                            tags.append('ai_technology')
                        if any(word in content_text for word in ['crypto', 'bitcoin', 'blockchain', 'web3', 'ethereum']):
                            tags.append('crypto_web3')
                        if any(word in content_text for word in ['media', 'journalism', 'publishing', 'content']):
                            tags.append('media_industry')
                        if any(word in content_text for word in ['business', 'entrepreneur', 'company', 'corporate']):
                            tags.append('business')
                        if any(word in content_text for word in ['technology', 'tech', 'software', 'app', 'digital']):
                            tags.append('technology')
                        if any(word in content_text for word in ['podcast', 'interview', 'conversation', 'discussion']):
                            tags.append('podcast_content')
                        
                        # Skip duplicate content
                        existing = db.query(Content).filter(
                            Content.source_id == source.id,
                            Content.content_url == link
                        ).first()
                        
                        if existing:
                            continue
                        
                        # Create Content object
                        content = Content(
                            title=title[:500] if title else 'No Title',
                            description=description[:1000] if description else '',
                            content_url=link,
                            content_text=description[:2000] if description else '',
                            author=author[:100] if author else None,
                            published_at=published_at,
                            tags=tags,
                            source_id=source.id
                        )
                        
                        db.add(content)
                        added_count += 1
                        
                    except Exception as e:
                        print(f"âš ï¸  Error processing entry: {e}")
                        continue
                
                if added_count > 0:
                    db.commit()
                    total_content += added_count
                    print(f"  âœ… Added {added_count} content items")
                else:
                    print(f"  âš ï¸  No content added")
                    
            except Exception as e:
                print(f"  âŒ Error processing {source.name}: {e}")
                db.rollback()
                continue
        
        print(f"\nğŸ‰ Content scraping complete!")
        print(f"ğŸ“Š Total content items added: {total_content}")
        
        # Show breakdown by category
        for source_type in ['blog', 'podcast', 'news', 'newsletter']:
            count = db.query(Content).join(Source).filter(Source.source_type == source_type).count()
            if count > 0:
                print(f"ğŸ“ {source_type.upper()}: {count} items")
            
        return total_content
        
    except Exception as e:
        print(f"âŒ Error during content scraping: {e}")
        db.rollback()
        return 0
    finally:
        db.close()

async def main():
    """Main function to fix SSL issues and populate content"""
    print("ğŸš€ FIXING SSL ISSUES AND ADDING WORKING CONTENT")
    print("=" * 60)
    
    # Initialize database
    init_db()
    
    # Step 1: Create working sources
    sources_added = await create_working_sources()
    
    if sources_added > 0:
        # Step 2: Scrape content from working sources
        content_added = await scrape_all_working_content()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ SSL FIX AND CONTENT POPULATION COMPLETE!")
        print(f"ğŸ“‚ Working sources added: {sources_added}")
        print(f"ğŸ“° Content items scraped: {content_added}")
        print("âœ… All categories now have working content for filters")
        print("\nğŸŒ Access your dashboard: http://localhost:8001")
        print("ğŸ” Try the category filters - they should work now!")
    else:
        print("âŒ Failed to create sources. Content scraping skipped.")

if __name__ == "__main__":
    asyncio.run(main())
